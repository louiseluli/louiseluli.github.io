#!/usr/bin/env python3
"""
Cinema Data Analysis - ENHANCED with Keywords & Deep Dive
"""

import json
import os
from collections import Counter
from datetime import datetime
from pathlib import Path
from typing import Optional, Dict
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

try:
    from sklearn.cluster import KMeans
    from sklearn.feature_extraction.text import TfidfVectorizer
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

WATCHLIST_CANDIDATES = [
    Path("data/Watched.csv"),
    Path("data/data/Watched.csv"),
    Path("data/Watched_enriched.csv"),
    Path("data/data/Watched_enriched.csv"),
    Path("data/enriched_movies.csv"),
    Path("data/data/enriched_movies.csv"),
]

PEOPLE_CANDIDATES = [
    Path("data/enriched_people.csv"),
    Path("data/data/enriched_people.csv"),
]

def resolve_watchlist(cli_arg: Optional[str] = None) -> Path:
    if cli_arg:
        p = Path(cli_arg).expanduser().resolve()
        if p.exists():
            return p
    for p in WATCHLIST_CANDIDATES:
        if p.exists():
            return p.resolve()
    raise FileNotFoundError("Could not find movie data CSV")

def resolve_people() -> Optional[Path]:
    for p in PEOPLE_CANDIDATES:
        if p.exists():
            return p.resolve()
    return None

def load_data(filepath: Optional[str] = None) -> pd.DataFrame:
    if filepath:
        fp = Path(filepath).expanduser().resolve()
    else:
        fp = resolve_watchlist(None)
    df = pd.read_csv(fp, encoding="utf-8-sig")
    print(f"‚úÖ Loaded {len(df)} movies from {fp}")
    return df

def load_people_data() -> Optional[pd.DataFrame]:
    people_path = resolve_people()
    if people_path:
        try:
            df = pd.read_csv(people_path, encoding="utf-8-sig")
            print(f"‚úÖ Loaded {len(df)} people/actors from {people_path}")
            return df
        except Exception as e:
            print(f"‚ö†Ô∏è  Could not load people data: {e}")
    return None

def clean_data(df: pd.DataFrame) -> pd.DataFrame:
    print("\nüßπ CLEANING DATA...")
    original_count = len(df)
    df = df.drop_duplicates()
    duplicates_removed = original_count - len(df)
    if duplicates_removed > 0:
        print(f"  ‚úì Removed {duplicates_removed} duplicates")
    
    column_mapping = {
        'Original Title': 'Title',
        'originalTitle': 'Title',
        'Runtime (mins)': 'Runtime',
        'runtimeMinutes': 'Runtime',
        'startYear': 'Year',
    }
    for old_col, new_col in column_mapping.items():
        if old_col in df.columns and new_col not in df.columns:
            df = df.rename(columns={old_col: new_col})
    
    if 'Year' in df.columns:
        df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
    if 'Runtime' in df.columns:
        df['Runtime'] = pd.to_numeric(df['Runtime'], errors='coerce')
    
    print(f"‚úÖ Cleaning complete! {len(df)} records ready\n")
    return df

def pick_col(df, *candidates, default=None):
    for c in candidates:
        if c in df.columns:
            return df[c]
    return pd.Series([pd.NA] * len(df)) if default is None else default

def to_num(s):
    return pd.to_numeric(s, errors="coerce")

def basic_stats(df):
    title = pick_col(df, 'Title', 'Original Title', 'originalTitle')
    runtime = to_num(pick_col(df, 'Runtime', 'Runtime (mins)', 'runtimeMinutes')).fillna(0)
    year = to_num(pick_col(df, 'Year', 'startYear'))
    earliest = int(year.min()) if year.notna().any() else None
    latest = int(year.max()) if year.notna().any() else None
    span = int(latest - earliest) if earliest and latest else None
    return {
        'total_movies': int(len(df)),
        'unique_movies': int(title.nunique(dropna=True)),
        'total_runtime_mins': float(runtime.sum()),
        'total_runtime_hours': round(float(runtime.sum()) / 60, 2),
        'total_runtime_days': round(float(runtime.sum()) / (60 * 24), 2),
        'avg_runtime': round(float(runtime.mean()), 2) if len(runtime) else None,
        'year_range': {'earliest': earliest, 'latest': latest, 'span': span}
    }

def genre_analysis(df):
    gcol = pick_col(df, 'genres_merged', 'Genres', 'genres')
    all_genres = []
    genre_by_movie = {}
    for idx, genres in gcol.dropna().astype(str).items():
        parts = []
        for sep in [',', '|']:
            if sep in genres:
                parts = [p.strip() for p in genres.split(sep)]
                break
        if not parts:
            parts = [genres.strip()]
        valid_parts = [p for p in parts if p and p.lower() != 'nan']
        all_genres.extend(valid_parts)
        genre_by_movie[idx] = valid_parts
    genre_counts = Counter(all_genres)
    genre_combos = Counter()
    for genres in genre_by_movie.values():
        if len(genres) >= 2:
            combo = ' + '.join(sorted(genres[:2]))
            genre_combos[combo] += 1
    return {
        'total_unique_genres': int(len(genre_counts)),
        'top_10_genres': dict(genre_counts.most_common(10)),
        'genre_distribution': dict(genre_counts),
        'top_genre_combinations': dict(genre_combos.most_common(10)),
        'genre_by_movie': genre_by_movie
    }

# NEW: KEYWORD ANALYSIS
def keyword_analysis(df):
    """Analyze keywords from movie metadata"""
    print("\nüîç ANALYZING KEYWORDS...")
    
    # Try different keyword columns
    keyword_cols = ['tmdb_keywords', 'keywords', 'plot_keywords']
    keyword_col = None
    for col in keyword_cols:
        if col in df.columns:
            keyword_col = col
            break
    
    if not keyword_col:
        print("  ‚ö†Ô∏è  No keyword column found")
        return {}
    
    all_keywords = []
    keyword_by_movie = {}
    
    for idx, keywords in df[keyword_col].dropna().astype(str).items():
        # Keywords might be JSON, comma-separated, or pipe-separated
        keywords_list = []
        
        # Try JSON parsing
        if keywords.startswith('[') or keywords.startswith('{'):
            try:
                import json
                parsed = json.loads(keywords)
                if isinstance(parsed, list):
                    keywords_list = [k.get('name', k) if isinstance(k, dict) else str(k) for k in parsed]
                elif isinstance(parsed, dict):
                    keywords_list = list(parsed.values())
            except:
                pass
        
        # Try comma/pipe separation
        if not keywords_list:
            for sep in [',', '|', ';']:
                if sep in keywords:
                    keywords_list = [k.strip() for k in keywords.split(sep)]
                    break
        
        # Clean and add
        keywords_list = [k for k in keywords_list if k and len(k) > 2 and k.lower() != 'nan']
        all_keywords.extend(keywords_list)
        if keywords_list:
            keyword_by_movie[idx] = keywords_list
    
    keyword_counts = Counter(all_keywords)
    
    print(f"  ‚úì Found {len(keyword_counts)} unique keywords")
    
    return {
        'total_unique_keywords': len(keyword_counts),
        'top_50_keywords': dict(keyword_counts.most_common(50)),
        'keyword_distribution': dict(keyword_counts),
        'keyword_by_movie': keyword_by_movie
    }

# NEW: DESCRIPTION/OVERVIEW ANALYSIS
def description_analysis(df):
    """Analyze movie descriptions/overviews using TF-IDF"""
    print("\nüìù ANALYZING DESCRIPTIONS...")
    
    # Try different description columns
    desc_cols = ['tmdb_overview', 'overview', 'description', 'plot', 'omdb_plot']
    desc_col = None
    for col in desc_cols:
        if col in df.columns:
            desc_col = col
            break
    
    if not desc_col:
        print("  ‚ö†Ô∏è  No description column found")
        return {}
    
    descriptions = df[desc_col].dropna().astype(str)
    
    if len(descriptions) < 10:
        print("  ‚ö†Ô∏è  Not enough descriptions for analysis")
        return {}
    
    if not HAS_SKLEARN:
        print("  ‚ö†Ô∏è  Install scikit-learn for advanced description analysis")
        # Simple word frequency
        all_words = []
        for desc in descriptions:
            words = desc.lower().split()
            words = [w.strip('.,!?;:') for w in words if len(w) > 4]
            all_words.extend(words)
        
        word_counts = Counter(all_words)
        return {
            'top_description_words': dict(word_counts.most_common(30)),
            'total_words_analyzed': len(all_words)
        }
    
    # Advanced TF-IDF analysis
    try:
        vectorizer = TfidfVectorizer(
            max_features=50,
            stop_words='english',
            min_df=2,
            max_df=0.8
        )
        
        tfidf_matrix = vectorizer.fit_transform(descriptions)
        feature_names = vectorizer.get_feature_names_out()
        
        # Get top terms by TF-IDF score
        tfidf_scores = tfidf_matrix.sum(axis=0).A1
        top_indices = tfidf_scores.argsort()[-30:][::-1]
        top_terms = {feature_names[i]: float(tfidf_scores[i]) for i in top_indices}
        
        print(f"  ‚úì Analyzed {len(descriptions)} descriptions")
        
        return {
            'top_tfidf_terms': top_terms,
            'total_descriptions_analyzed': len(descriptions),
            'unique_terms': len(feature_names)
        }
    except Exception as e:
        print(f"  ‚ö†Ô∏è  TF-IDF analysis error: {e}")
        return {}

def decade_analysis(df):
    year = to_num(pick_col(df, 'Year', 'startYear'))
    decade = ((year // 10) * 10).dropna().astype(int)
    tmp = df.copy()
    tmp['Decade'] = decade
    decade_counts = tmp['Decade'].value_counts().sort_index()
    runtime = to_num(pick_col(tmp, 'Runtime', 'Runtime (mins)', 'runtimeMinutes')).fillna(0)
    decade_stats = {}
    for d in decade_counts.index:
        mask = tmp['Decade'] == d
        decade_stats[int(d)] = {
            'count': int(mask.sum()),
            'total_runtime_hours': round(float(runtime[mask].sum()) / 60, 2),
        }
    return decade_stats

def director_analysis(df):
    if 'Directors' not in df.columns:
        return {}
    all_directors = []
    for directors in df['Directors'].dropna().astype(str):
        all_directors.extend([d.strip() for d in directors.split(',') if d.strip()])
    director_counts = Counter(all_directors)
    title = pick_col(df, 'Title', 'Original Title', 'originalTitle')
    runtime = to_num(pick_col(df, 'Runtime', 'Runtime (mins)', 'runtimeMinutes')).fillna(0)
    top_directors = {}
    for director, count in director_counts.most_common(20):
        director_movies = df[df['Directors'].str.contains(director, na=False, regex=False)]
        top_directors[director] = {
            'movie_count': int(count),
            'total_runtime_hours': round(float(runtime.loc[director_movies.index].sum()) / 60, 2),
            'movies': title.loc[director_movies.index].dropna().astype(str).tolist()[:10],
        }
    return top_directors

def actor_analysis(people_df, movies_df):
    if people_df is None or len(people_df) == 0:
        return {}
    print("\nüé≠ ANALYZING ACTORS...")
    if 'primaryProfession' in people_df.columns:
        actors = people_df[people_df['primaryProfession'].str.contains('actor|actress', case=False, na=False)].copy()
    else:
        actors = people_df.copy()
    print(f"  Found {len(actors)} actors")
    actor_stats = {}
    for idx, row in actors.head(100).iterrows():
        name = row.get('primaryName', 'Unknown')
        birth_year = row.get('birthYear')
        known_for = row.get('knownForTitles', '')
        if pd.notna(known_for):
            known_titles = str(known_for).split(',')
            matches = 0
            if 'Const' in movies_df.columns:
                our_ids = set(movies_df['Const'].dropna().astype(str))
                matches = len([t for t in known_titles if t.strip() in our_ids])
            if matches > 0 or len(actor_stats) < 50:
                actor_stats[name] = {
                    'appearances': matches,
                    'birth_year': int(birth_year) if pd.notna(birth_year) and birth_year != '\\N' else None,
                    'known_for_count': len(known_titles)
                }
    sorted_actors = dict(sorted(actor_stats.items(), key=lambda x: x[1]['appearances'], reverse=True)[:30])
    return {'total_actors': len(actors), 'top_actors': sorted_actors, 'actors_analyzed': len(actor_stats)}

def ml_analysis(df, genre_data):
    if not HAS_SKLEARN:
        return {'note': 'Install scikit-learn for ML features'}
    print("\nü§ñ PERFORMING ML ANALYSIS...")
    results = {}
    try:
        print("  üìä Clustering movies by genre...")
        genre_by_movie = genre_data.get('genre_by_movie', {})
        all_genres = list(set(g for genres in genre_by_movie.values() for g in genres))
        if len(all_genres) > 0 and len(genre_by_movie) > 10:
            genre_matrix = []
            movie_indices = []
            for idx, genres in genre_by_movie.items():
                if len(genres) > 0:
                    row = [1 if g in genres else 0 for g in all_genres]
                    genre_matrix.append(row)
                    movie_indices.append(idx)
            if len(genre_matrix) > 10:
                genre_matrix = np.array(genre_matrix)
                n_clusters = min(8, len(genre_matrix) // 50)
                if n_clusters >= 2:
                    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                    clusters = kmeans.fit_predict(genre_matrix)
                    cluster_genres = {}
                    for i in range(n_clusters):
                        cluster_mask = clusters == i
                        cluster_movies = [movie_indices[j] for j, mask in enumerate(cluster_mask) if mask]
                        cluster_genre_counts = Counter()
                        for idx in cluster_movies[:50]:
                            cluster_genre_counts.update(genre_by_movie.get(idx, []))
                        top_genres = [g for g, _ in cluster_genre_counts.most_common(3)]
                        cluster_name = ' + '.join(top_genres) if top_genres else f'Cluster {i}'
                        cluster_genres[cluster_name] = {
                            'size': int(cluster_mask.sum()),
                            'percentage': round(float(cluster_mask.sum() / len(clusters) * 100), 2)
                        }
                    results['genre_clustering'] = cluster_genres
                    print(f"  ‚úì Created {n_clusters} genre clusters")
    except Exception as e:
        print(f"  ‚ö†Ô∏è  Clustering error: {e}")
    return results

def prepare_graph_data(df, genres, decades, directors):
    year = to_num(pick_col(df, 'Year', 'startYear')).dropna()
    year_counts = year.value_counts().sort_index()
    runtime = to_num(pick_col(df, 'Runtime', 'Runtime (mins)', 'runtimeMinutes')).dropna()
    runtime_bins = pd.cut(runtime, bins=[0, 60, 90, 120, 150, 180, 600], 
                          labels=['<60', '60-90', '90-120', '120-150', '150-180', '180+'])
    runtime_dist = runtime_bins.value_counts().sort_index()
    return {
        'timeline': {
            'years': [int(y) for y in year_counts.index],
            'counts': [int(c) for c in year_counts.values]
        },
        'runtime_histogram': {
            'bins': [str(b) for b in runtime_dist.index],
            'counts': [int(c) for c in runtime_dist.values]
        }
    }

def generate_insights(df):
    print("\n" + "="*60)
    print("üé¨ CINEMA ANALYTICS - ENHANCED with KEYWORDS")
    print("="*60)
    
    stats = basic_stats(df)
    genres = genre_analysis(df)
    keywords = keyword_analysis(df)
    descriptions = description_analysis(df)
    decades = decade_analysis(df)
    directors = director_analysis(df)
    
    people_df = load_people_data()
    actors = {}
    if people_df is not None:
        actors = actor_analysis(people_df, df)
    
    ml_results = ml_analysis(df, genres)
    graph_data = prepare_graph_data(df, genres, decades, directors)
    
    print(f"\n‚úÖ Analysis complete!")
    print(f"üìä {stats['total_movies']} movies")
    print(f"üé≠ {genres['total_unique_genres']} genres")
    if keywords:
        print(f"üîç {keywords.get('total_unique_keywords', 0)} keywords")
    if descriptions:
        print(f"üìù {descriptions.get('total_descriptions_analyzed', 0)} descriptions analyzed")
    print(f"üé¨ {len(directors)} directors")
    if actors.get('total_actors'):
        print(f"üåü {actors['total_actors']} actors")
    
    return {
        'basic_stats': stats,
        'genres': {k: v for k, v in genres.items() if k != 'genre_by_movie'},
        'keywords': keywords if keywords else {},
        'descriptions': descriptions if descriptions else {},
        'decades': decades,
        'directors': directors,
        'actors': actors,
        'ml_insights': ml_results,
        'graph_data': graph_data,
        'generated_at': datetime.now().isoformat()
    }

def save_insights(insights, output_path='data/data/cinema_insights.json'):
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(insights, f, indent=2, ensure_ascii=False)
    print(f"\n‚úÖ Insights saved to: {output_path}")

def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--input', type=str)
    parser.add_argument('--output', type=str, default='data/data/cinema_insights.json')
    parser.add_argument('--no-clean', action='store_true')
    args = parser.parse_args()
    try:
        df = load_data(args.input)
        if not args.no_clean:
            df = clean_data(df)
        insights = generate_insights(df)
        save_insights(insights, args.output)
        print("\nüéâ Dashboard ready!")
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    return 0

if __name__ == '__main__':
    exit(main())